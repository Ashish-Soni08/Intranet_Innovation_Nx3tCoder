{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import (JINA_API_KEY,\n",
    "                       EMBEDDING_MODEL,\n",
    "                       LLM,\n",
    "                       OPENAI_API_KEY,\n",
    "                       QDRANT_CLUSTER,\n",
    "                       QDRANT_API_KEY\n",
    "                       )\n",
    "\n",
    "from utils import num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Load data from Directory\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a computer scientist, entrepreneur, and venture capitalist. He is best known for co-founding the startup accelerator Y Combinator and for his work as an essayist and author on topics related to technology, startups, and entrepreneurship. Graham has been influential in the tech industry and is considered a thought leader in the startup community.\n"
     ]
    }
   ],
   "source": [
    "response = OpenAI(model = LLM, api_key = OPENAI_API_KEY).complete(\"Paul Graham is \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST EMBEDDING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "\n",
    "embed_model = JinaEmbedding(api_key=JINA_API_KEY, model=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[-0.004714012, -0.16542053, -0.040442467, 0.0311203, 0.31619263]\n",
      "768\n",
      "[0.3563029, 0.29810587, -0.07292429, 0.026819864, 0.03765996]\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"This is the text to embed\")\n",
    "\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])\n",
    "\n",
    "embeddings = embed_model.get_text_embedding(\"Heute ist Freitag und ich habe keine Lust zu arbeiten\")\n",
    "print(len(embeddings))\n",
    "print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am awesome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(num_tokens(text, model = LLM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quoting Jonathan from the N3XTCODER TEAM**\n",
    "\n",
    "`We have prepared a synthetic dataset based on real documents and structure from a municipal authority. This dataset is simplified to be text-only (no pdfs, docs) and emulates some of the folder structure of the real data. It also includes some evaluation questions within the file eval_questions_v2.txt.`\n",
    "\n",
    "`Everything in this dataset is fiction, and it obviously has shortcomings with regards to proving that a solution will work on the real data. However we hope it will help in the design of your solutions. If a solution is promising, then further evaluation work using real data from our challenge partner, can be undertaken at the end of the programme.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loaders\n",
    "\n",
    "Before choosing an LLM to act on your data, you need to load it. LlamaIndex does this via data connectors, also called **Reader**.\n",
    "Data connectors ingest data from different data sources and format the data into **Document** objects. A **Document** is a collection of data(text) and metadata about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_dir: str = \"data/raw/v2_unzipped/txt\", file_ext: List[str] = [\".txt\"], recursive: bool = True, filename_as_id: bool = True) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from a specified directory with given file extensions.\n",
    "\n",
    "    This function utilizes the SimpleDirectoryReader to read files from the given \n",
    "    directory and create documents. The function supports reading files with \n",
    "    specified extensions and can optionally read files recursively from subdirectories.\n",
    "    Additionally, it can use filenames as document IDs.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_dir : str, optional\n",
    "        The path to the directory from which to load the files. Defaults to \"data/raw/v2_unzipped/txt\".\n",
    "    file_ext : List[str], optional\n",
    "        A list of file extensions to filter the files that need to be read. Defaults to [\".txt\"].\n",
    "    recursive : bool, optional\n",
    "        If True, the reader will include files from subdirectories recursively. Defaults to True.\n",
    "    filename_as_id : bool, optional\n",
    "        If True, the filenames will be used as document IDs. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    List[Document]\n",
    "        A list of documents read from the specified directory.\n",
    "    \n",
    "    Example:\n",
    "    -------\n",
    "    >>> documents = load_data(input_dir=\"data/my_texts\", file_ext=[\".txt\", \".md\"], recursive=False, filename_as_id=False)\n",
    "    Loaded 10 docs\n",
    "    >>> print(len(documents))\n",
    "    10\n",
    "    \"\"\"\n",
    "    \n",
    "    # SimpleDirectoryReader creates documents out of every file in a given directory\n",
    "    reader = SimpleDirectoryReader(input_dir=input_dir,\n",
    "                                   required_exts=file_ext,\n",
    "                                   recursive=recursive,\n",
    "                                   filename_as_id=filename_as_id\n",
    "                                   )\n",
    "    documents = reader.load_data()\n",
    "    print(f\"Loaded {len(documents)} docs\")\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44 docs\n"
     ]
    }
   ],
   "source": [
    "docs = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/Intranet_Innovation_Nx3tCoder/data/raw/v2_unzipped/txt/Büro Bürgermeister/Da01-02_Presse.txt\n",
      " \n",
      "Guidelines for Pre\n",
      "/workspaces/Intranet_Innovation_Nx3tCoder/data/raw/v2_unzipped/txt/Büro Bürgermeister/da01-03.txt\n",
      "13 \n",
      " \n",
      "Eine neue DSFA\n",
      "/workspaces/Intranet_Innovation_Nx3tCoder/data/raw/v2_unzipped/txt/ENNI Stadt und Service Niederrhein AöR/daenni01_dienstleistungsrahmenvertrag.txt\n",
      "Max Mustermann\n",
      "01.02\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:3]:\n",
    "    print(doc.metadata['file_path'])\n",
    "    print(doc.text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nGuidelines for Press and Public Relations of the City Administration of Fictitiousville\\nThe press and public relations of the City Administration of Fictitiousville primarily aims to publicize the services provided by all organizational units as shown in the Departmental Distribution Plan. The goal is to create a positive image of the entire administration. Press and public relations are understood as professional communication with the aim of strengthening and improving the image of the city and administration. This should be achieved as a unified voice.\\n\\nThe press work of Fictitiousville is to be proactively shaped. This includes reporting on public relevant issues at an early stage.\\n\\nTo achieve these goals, the following regulations are made:\\n\\n1. Responsibilities\\nThe responsibility for press and public relations lies with the Mayor. The coordination, central planning, and implementation are taken over by the Department 1.1 - Press Office. The content is coordinated with the Mayor or, depending on the topic, with the management of the respective department or with the management of the responsible organizational unit or the departmental management. The Press Office informs the Mayor or the departmental management about the results.\\n\\nThis procedure does not apply to the daily news coverage of fire department activities.\\n\\n2. Press Releases and Press Conversations\\nPress work is the factual, comprehensive, and continuous information of the public through the media, the city of Fictitiousville's website, and other publications if necessary. This includes writing press releases and other texts that are published, as well as conducting press conversations. The editorial responsibility for the internet remains with the Department 3/ Central Services Department.\\n\\nPress releases, invitations to press conversations, and other written information for the media are written and sent by the Press Office in coordination with the organizational units.\\n\\nPress consultations are journalistically prepared and moderated by the Press Office in coordination with the Mayor or departmental management and the organizational units. The organization (e.g., room reservation) is the responsibility of the respective department.\\n\\n3. Media Inquiries\\nThe Press Office coordinates the response to media inquiries with the Mayor or departmental management and/ or the management of the organizational units or departmental departments. The organizational units provide the Press Office with information so that a quick response to the request is possible. The feedback to the departmental management is given by the Press Office or the management of the organizational units or departments. In addition, information to the media is reserved for the departmental management in their areas of responsibility. The management of the organizational units is only authorized to provide this on a case-by-case basis after prior approval from the Mayor or the responsible departmental management. The Press Office must be informed in all cases about information provided to the media. If a topic is of particular importance, the Mayor must be informed before providing information.\\n\\nNotifications of events, courses, etc. of cultural institutions can be coordinated between the Press Office and the organizational units in consultation with the departmental management.\\n\\nTo ensure that the Press Office is prepared, they must be informed in a timely manner about public relevant topics. This also applies to topics that may be subject to media inquiries due to intense public interest.\\n\\n4. Publications, Advertisements, Events\\nTo ensure a uniform appearance, the Press Office must be involved in all publications intended for the public. Flyers, brochures, etc. are prepared by the respective organizational units and remain under their editorial responsibility.\\n\\nOfficial advertisements (e.g., invitations to tender, announcements, obituaries) are prepared by the respective organizational units and placed by the Press Office.\\n\\nThe Press Office must be involved in the planning of public information events. (Excluded are public participation procedures, etc.)\\n\\n5. Final Remarks\\nThese regulations come into effect on January 01, 2009. The directive of November 7, 1978 and all conflicting orders will be repealed at the same time.\\n\\nFictitiousville, December 16, 2008\\nSigned,\\nJohn Smith\\nMayor\\n01/02\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "identify_language_template_str = (\n",
    "    \"\"\"\n",
    "    # ROLE\n",
    "    Act as an Expert Document Language Detector, specialized in identifying the language of a given text.\n",
    "    \n",
    "    # CONTEXT\n",
    "    You are provided with text that may contain content in English, German, or a mix of both languages.\n",
    "    Your task is to identify the predominant language of the text, determining whether it is mostly in English or German.\n",
    "\n",
    "    # RESPONSIBILITY\n",
    "    The text is as follows:\\n{text}\\n\n",
    "    Please identify the predominant language as either 'English' or 'German'.\n",
    "\n",
    "    # RULES\n",
    "    1. Output the predominant language as a one-word string: either 'English' or 'German'.\n",
    "    2. If the text contains a mix of both languages, determine which language is more predominant.\n",
    "    3. Ignore the presence of a few words or sentences in German or English in the text of a document. Focus on identifying the main language.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilingual_text_identifier = PromptTemplate(identify_language_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/2009\n"
     ]
    }
   ],
   "source": [
    "response = OpenAI(model = LLM, api_key = OPENAI_API_KEY)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata(documents: List[Document]) -> List[Document]:\n",
    "    for doc in documents:\n",
    "\n",
    "        # Extracting the directory path\n",
    "        directory_path = os.path.dirname(doc.metadata['file_path'])\n",
    "        folder_name = os.path.basename(directory_path)\n",
    "        \n",
    "        # Adding the directory path as a new metadata field\n",
    "        doc.metadata['folder_name'] = folder_name\n",
    "        \n",
    "        # Adding the number of tokens in the text as a new metadata field\n",
    "        tokens = num_tokens(doc.text, model = LLM)\n",
    "        doc.metadata['num_tokens'] = tokens\n",
    "\n",
    "        # Adding the language of the text as a new metadata field\n",
    "        language \n",
    "        doc.metadata['language'] = \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    return {\"file_path\": file_path,\n",
    "            \"num_tokens\": num_tokens(file_path, model = LLM),\n",
    "            \"file_name\": file_path.split(\"/\")[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata = {'file_path': '/workspaces/Intranet_Innovation_Nx3tCoder/data/raw/v2_unzipped/txt/ENNI Stadt und Service Niederrhein AöR/daenni01_dienstleistungsrahmenvertrag.txt',\n",
    " 'file_name': 'daenni01_dienstleistungsrahmenvertrag.txt',\n",
    " 'file_type': 'text/plain',\n",
    " 'file_size': 6215,\n",
    " 'creation_date': '2024-05-10',\n",
    " 'last_modified_date': '2024-04-30'}\n",
    "\n",
    "print(directory_path)\n",
    "# Extracting the folder name\n",
    "folder_name = os.path.basename(directory_path)\n",
    "\n",
    "print(folder_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "After loading the data, you need to process and transform it before putting it into a storage system. Transformations include \n",
    "- chunking\n",
    "- extracting metadata\n",
    "- embedding each chunk\n",
    "\n",
    "This is necessary to make sure that the data can be retrieved and used optimally by the LLM.\n",
    "\n",
    "\n",
    "Transformation input/outputs are Node objects (a Document is a subclass of a Node). Transformations can also be stacked and reordered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])\n",
    "\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE THIS IN MARKDOWN FILE TO EXPLAIN NODE\n",
    "Under the hood, this splits your Document into Node objects, which are similar to Documents (they contain text and metadata) but have a relationship to their parent Document.\n",
    "\n",
    "If you want to customize core components, like the text splitter, through this abstraction you can pass in a custom transformations list or apply to the global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/workspaces/Intranet_Innovation_Nx3tCoder/data/raw/v2_unzipped/txt/Büro Bürgermeister/Da01-02_Presse.txt',\n",
       " 'file_name': 'Da01-02_Presse.txt',\n",
       " 'file_type': 'text/plain',\n",
       " 'file_size': 4398,\n",
       " 'creation_date': '2024-05-10',\n",
       " 'last_modified_date': '2024-04-30'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(url=QDRANT_CLUSTER, api_key=QDRANT_API_KEY)\n",
    "\n",
    "vector_store_hybrid = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"hacker-news-hybrid\",\n",
    "    enable_hybrid=True,\n",
    "    batch_size=16\n",
    "    )  # this is important for the ingestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL ETL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.jinaai import JinaEmbedding\n",
    "from llama_index.postprocessor.jinaai_rerank import JinaRerank\n",
    "\n",
    "jina_rerank = JinaRerank(api_key=JINA_API_KEY, top_n=5, )\n",
    "\n",
    "embed_model = JinaEmbedding(api_key=JINA_API_KEY, model=EMBEDDING_MODEL, embed_batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intranet-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
